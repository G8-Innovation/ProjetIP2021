{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date :** Created on Thursday January 7 2021\n",
    "\n",
    "**Group 8 - Innovation**\n",
    "\n",
    "**Word2Vec_notebook_v0** \n",
    "\n",
    "**@author :** Damien Sonneville, Samba Seye. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description :** This notebook will calculate word vectors according to the Word2Vec method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Install / Download / Import Librairy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install librairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Text librairy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Word2Vec Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(text : str) -> list :\n",
    "    \"\"\" Documentation\n",
    "    \n",
    "        Parameters:\n",
    "            - text : represents each distinct sentence \n",
    "                with a particular list of numbers \n",
    "        \n",
    "        Output (if exists) :\n",
    "            - vectors : list of vector for each sentence\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # My initialized vectors list\n",
    "    vectors : list = []\n",
    "        \n",
    "    # Iterate through each sentence in the file \n",
    "    for sent in sent_tokenize(text):\n",
    "        \n",
    "        # My initialized word list\n",
    "        temp : list = []\n",
    "            \n",
    "        # Tokenize the sentence into words \n",
    "        for word in word_tokenize(sent): \n",
    "            \n",
    "            temp.append(word.lower())\n",
    "        \n",
    "        # Create CBOW model \n",
    "        model = gensim.models.Word2Vec(temp, min_count = 1, \\  \n",
    "                              size = 100, window = 5)\n",
    "        \n",
    "        # Create vector of sentences \n",
    "        sent_vector = sum(model.wv.get_vector(word) for word in model.wv.vocab)\n",
    "        \n",
    "        vectors.append(sent_vector)\n",
    "    \n",
    "    # Return my vector list\n",
    "    return vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
